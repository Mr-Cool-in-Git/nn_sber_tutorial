{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 36\n",
    "HIDDEN_SIZE = 25\n",
    "OUTPUT_SIZE = 5\n",
    "LEARNING_RATE = 1e-2\n",
    "EPOCHS = 400\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    # Конструктор, где считаем датасет\n",
    "    def __init__(self):\n",
    "        X = pd.read_csv('./data/X_cat.csv', sep='\\t', index_col=0)\n",
    "        target = pd.read_csv('./data/y_cat.csv', sep='\\t', index_col=0, names=['status'])  # header=-1,\n",
    "\n",
    "        weekday_columns = ['Weekday_0', 'Weekday_1', 'Weekday_2',\n",
    "                           'Weekday_3', 'Weekday_4', 'Weekday_5', 'Weekday_6']\n",
    "        weekdays = np.argmax(X[weekday_columns].values, axis=1)\n",
    "\n",
    "        X.drop(weekday_columns, axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "        X['Weekday_cos'] = np.cos(2 * np.pi / 7.) * weekdays\n",
    "        X['Weekday_sin'] = np.sin(2 * np.pi / 7.) * weekdays\n",
    "\n",
    "        X['Hour_cos'] = np.cos(2 * np.pi / 24.) * X['Hour'].values\n",
    "        X['Hour_sin'] = np.sin(2 * np.pi / 24.) * X['Hour'].values\n",
    "\n",
    "        X['Month_cos'] = np.cos(2 * np.pi / 12.) * X['Month'].values\n",
    "        X['Month_sin'] = np.sin(2 * np.pi / 12.) * X['Month'].values\n",
    "\n",
    "        X['Gender'] = np.argmax(X[['Sex_Female', 'Sex_Male', 'Sex_Unknown']].values, axis=1)\n",
    "\n",
    "        X.drop(['Sex_Female', 'Sex_Male', 'Sex_Unknown'], axis=1, inplace=True)\n",
    "\n",
    "        print(X.shape)\n",
    "        print(X.head())\n",
    "\n",
    "        target = target.iloc[:, :].values\n",
    "        target[target == 'Died'] = 'Euthanasia'\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        self.y = le.fit_transform(target)\n",
    "\n",
    "        self.X = X.values\n",
    "\n",
    "        self.columns = X.columns.values\n",
    "\n",
    "        self.embedding_column = 'Gender'\n",
    "        self.nrof_emb_categories = 3\n",
    "        self.numeric_columns = ['IsDog', 'Age', 'HasName', 'NameLength', 'NameFreq', 'MixColor', 'ColorFreqAsIs',\n",
    "                                'ColorFreqBase', 'TabbyColor', 'MixBreed', 'Domestic', 'Shorthair', 'Longhair',\n",
    "                                'Year', 'Day',  'Breed_Chihuahua Shorthair Mix', 'Breed_Domestic Medium Hair Mix',\n",
    "                                'Breed_Domestic Shorthair Mix', 'Breed_German Shepherd Mix', 'Breed_Labrador Retriever Mix',\n",
    "                                 'Breed_Pit Bull Mix', 'Breed_Rare',\n",
    "                                'SexStatus_Flawed', 'SexStatus_Intact', 'SexStatus_Unknown',\n",
    "                                'Weekday_cos', 'Weekday_sin', 'Hour_cos', 'Hour_sin',\n",
    "                                'Month_cos', 'Month_sin']\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # Переопределяем метод,\n",
    "    # который достает по индексу наблюдение из датасет\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.X[idx, :]\n",
    "\n",
    "        row = {col: torch.tensor(row[i]) for i, col in enumerate(self.columns)}\n",
    "\n",
    "        return row, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, nrof_cat, emb_dim,\n",
    "                 emb_columns, numeric_columns):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.emb_columns = emb_columns\n",
    "        self.numeric_columns = numeric_columns\n",
    "\n",
    "        self.emb_layer = torch.nn.Embedding(nrof_cat, emb_dim)\n",
    "\n",
    "        self.feature_bn = torch.nn.BatchNorm1d(input_size)\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear1.apply(self.init_weights)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear2.apply(self.init_weights)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            # m.bias.data.fill_(0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb_output = self.emb_layer(torch.tensor(x[self.emb_columns], dtype=torch.int64))\n",
    "        numeric_feats = torch.tensor(pd.DataFrame(x)[self.numeric_columns].values, dtype=torch.float32)\n",
    "\n",
    "        concat_input = torch.cat([numeric_feats, emb_output], dim=1)\n",
    "        output = self.feature_bn(concat_input)\n",
    "\n",
    "        output = self.linear1(output)\n",
    "        output = self.bn1(output)\n",
    "        output = torch.relu(output)\n",
    "\n",
    "        output = self.linear2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = torch.relu(output)\n",
    "\n",
    "        output = self.linear3(output)\n",
    "        predictions = torch.softmax(output, dim=1)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_train(model, train_loader):\n",
    "    step = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        for features, label in train_loader:\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(features)\n",
    "            # Calculate error and backpropagate\n",
    "            loss = criterion(output, label.type(torch.LongTensor))\n",
    "            loss.backward()\n",
    "            acc = accuracy(output, label.type(torch.LongTensor)).item()\n",
    "\n",
    "            # Update weights with gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print('EPOCH %d STEP %d : train_loss: %f train_acc: %f' %\n",
    "                      (epoch, step, loss.item(), acc))\n",
    "\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26729, 34)\n",
      "   IsDog    Age  HasName  NameLength  NameFreq  MixColor  ColorFreqAsIs  \\\n",
      "0      1  365.0        1           7  0.000157         1       0.032919   \n",
      "1      0  365.0        1           5  0.000655         0       0.008092   \n",
      "2      1  730.0        1           6  0.000052         1       0.026293   \n",
      "3      0   21.0        0           7  0.285871         0       0.000471   \n",
      "4      1  730.0        0           7  0.285871         0       0.023831   \n",
      "\n",
      "   ColorFreqBase  TabbyColor  MixBreed  ...  SexStatus_Flawed  \\\n",
      "0       0.463624           0         1  ...                 1   \n",
      "1       0.015005           1         1  ...                 1   \n",
      "2       0.357521           0         1  ...                 1   \n",
      "3       0.058418           0         1  ...                 0   \n",
      "4       0.075353           0         0  ...                 1   \n",
      "\n",
      "   SexStatus_Intact  SexStatus_Unknown  Weekday_cos  Weekday_sin   Hour_cos  \\\n",
      "0                 0                  0     1.246980     1.563663  13.877134   \n",
      "1                 0                  0     3.740939     4.690989   8.435752   \n",
      "2                 0                  0     3.117449     3.909157   9.144098   \n",
      "3                 1                  0     2.493959     3.127326  14.633776   \n",
      "4                 0                  0     2.493959     3.127326   8.564542   \n",
      "\n",
      "   Hour_sin  Month_cos  Month_sin  Gender  \n",
      "0  3.718367   1.732051        1.0       1  \n",
      "1  2.260353   8.660254        5.0       0  \n",
      "2  2.450154   0.866025        0.5       1  \n",
      "3  3.921109   6.062178        3.5       1  \n",
      "4  2.294862   9.526279        5.5       1  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "animal_dataset = CustomDataset()\n",
    "train_loader = data_utils.DataLoader(dataset=animal_dataset,\n",
    "                                     batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = MLPNet(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, animal_dataset.nrof_emb_categories,\n",
    "               EMBEDDING_SIZE,\n",
    "               animal_dataset.embedding_column, animal_dataset.numeric_columns)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "accuracy = Accuracy()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 STEP 100 : train_loss: 1.240136 train_acc: 0.648438\n",
      "EPOCH 1 STEP 200 : train_loss: 1.246385 train_acc: 0.656250\n",
      "EPOCH 2 STEP 300 : train_loss: 1.262766 train_acc: 0.617188\n",
      "EPOCH 3 STEP 400 : train_loss: 1.242931 train_acc: 0.656250\n",
      "EPOCH 4 STEP 500 : train_loss: 1.219584 train_acc: 0.671875\n",
      "EPOCH 5 STEP 600 : train_loss: 1.276034 train_acc: 0.621094\n",
      "EPOCH 6 STEP 700 : train_loss: 1.278863 train_acc: 0.601562\n",
      "EPOCH 7 STEP 800 : train_loss: 1.271471 train_acc: 0.632812\n",
      "EPOCH 8 STEP 900 : train_loss: 1.227869 train_acc: 0.679688\n",
      "EPOCH 9 STEP 1000 : train_loss: 1.230511 train_acc: 0.664062\n",
      "EPOCH 10 STEP 1100 : train_loss: 1.261269 train_acc: 0.632812\n",
      "EPOCH 11 STEP 1200 : train_loss: 1.244008 train_acc: 0.660156\n",
      "EPOCH 12 STEP 1300 : train_loss: 1.227206 train_acc: 0.671875\n",
      "EPOCH 13 STEP 1400 : train_loss: 1.233298 train_acc: 0.660156\n",
      "EPOCH 14 STEP 1500 : train_loss: 1.219231 train_acc: 0.687500\n",
      "EPOCH 15 STEP 1600 : train_loss: 1.218623 train_acc: 0.687500\n",
      "EPOCH 16 STEP 1700 : train_loss: 1.219219 train_acc: 0.691406\n",
      "EPOCH 17 STEP 1800 : train_loss: 1.235298 train_acc: 0.667969\n",
      "EPOCH 18 STEP 1900 : train_loss: 1.215809 train_acc: 0.679688\n",
      "EPOCH 19 STEP 2000 : train_loss: 1.216384 train_acc: 0.683594\n",
      "EPOCH 19 STEP 2100 : train_loss: 1.199215 train_acc: 0.704762\n",
      "EPOCH 20 STEP 2200 : train_loss: 1.182443 train_acc: 0.722656\n",
      "EPOCH 21 STEP 2300 : train_loss: 1.173672 train_acc: 0.734375\n",
      "EPOCH 22 STEP 2400 : train_loss: 1.169218 train_acc: 0.738281\n",
      "EPOCH 23 STEP 2500 : train_loss: 1.192790 train_acc: 0.707031\n",
      "EPOCH 24 STEP 2600 : train_loss: 1.179355 train_acc: 0.714844\n",
      "EPOCH 25 STEP 2700 : train_loss: 1.228121 train_acc: 0.675781\n",
      "EPOCH 26 STEP 2800 : train_loss: 1.202118 train_acc: 0.703125\n",
      "EPOCH 27 STEP 2900 : train_loss: 1.229605 train_acc: 0.664062\n",
      "EPOCH 28 STEP 3000 : train_loss: 1.227362 train_acc: 0.679688\n",
      "EPOCH 29 STEP 3100 : train_loss: 1.243707 train_acc: 0.660156\n",
      "EPOCH 30 STEP 3200 : train_loss: 1.225469 train_acc: 0.667969\n",
      "EPOCH 31 STEP 3300 : train_loss: 1.228791 train_acc: 0.667969\n",
      "EPOCH 32 STEP 3400 : train_loss: 1.269752 train_acc: 0.617188\n",
      "EPOCH 33 STEP 3500 : train_loss: 1.218952 train_acc: 0.687500\n",
      "EPOCH 34 STEP 3600 : train_loss: 1.209965 train_acc: 0.695312\n",
      "EPOCH 35 STEP 3700 : train_loss: 1.222859 train_acc: 0.671875\n",
      "EPOCH 36 STEP 3800 : train_loss: 1.243547 train_acc: 0.660156\n",
      "EPOCH 37 STEP 3900 : train_loss: 1.263249 train_acc: 0.636719\n",
      "EPOCH 38 STEP 4000 : train_loss: 1.203490 train_acc: 0.707031\n",
      "EPOCH 39 STEP 4100 : train_loss: 1.253364 train_acc: 0.648438\n",
      "EPOCH 39 STEP 4200 : train_loss: 1.237758 train_acc: 0.647619\n",
      "EPOCH 40 STEP 4300 : train_loss: 1.204501 train_acc: 0.703125\n",
      "EPOCH 41 STEP 4400 : train_loss: 1.197170 train_acc: 0.703125\n",
      "EPOCH 42 STEP 4500 : train_loss: 1.169381 train_acc: 0.726562\n",
      "EPOCH 43 STEP 4600 : train_loss: 1.255013 train_acc: 0.636719\n",
      "EPOCH 44 STEP 4700 : train_loss: 1.176153 train_acc: 0.726562\n",
      "EPOCH 45 STEP 4800 : train_loss: 1.244563 train_acc: 0.660156\n",
      "EPOCH 46 STEP 4900 : train_loss: 1.244042 train_acc: 0.664062\n",
      "EPOCH 47 STEP 5000 : train_loss: 1.197899 train_acc: 0.707031\n",
      "EPOCH 48 STEP 5100 : train_loss: 1.223478 train_acc: 0.683594\n",
      "EPOCH 49 STEP 5200 : train_loss: 1.194948 train_acc: 0.714844\n",
      "EPOCH 50 STEP 5300 : train_loss: 1.163687 train_acc: 0.742188\n",
      "EPOCH 51 STEP 5400 : train_loss: 1.210792 train_acc: 0.691406\n",
      "EPOCH 52 STEP 5500 : train_loss: 1.283252 train_acc: 0.621094\n",
      "EPOCH 53 STEP 5600 : train_loss: 1.259500 train_acc: 0.644531\n",
      "EPOCH 54 STEP 5700 : train_loss: 1.178736 train_acc: 0.726562\n",
      "EPOCH 55 STEP 5800 : train_loss: 1.210205 train_acc: 0.679688\n",
      "EPOCH 56 STEP 5900 : train_loss: 1.162050 train_acc: 0.746094\n",
      "EPOCH 57 STEP 6000 : train_loss: 1.181285 train_acc: 0.718750\n",
      "EPOCH 58 STEP 6100 : train_loss: 1.192135 train_acc: 0.714844\n",
      "EPOCH 59 STEP 6200 : train_loss: 1.227347 train_acc: 0.679688\n",
      "EPOCH 59 STEP 6300 : train_loss: 1.157488 train_acc: 0.752381\n",
      "EPOCH 60 STEP 6400 : train_loss: 1.180486 train_acc: 0.718750\n",
      "EPOCH 61 STEP 6500 : train_loss: 1.259531 train_acc: 0.648438\n",
      "EPOCH 62 STEP 6600 : train_loss: 1.190404 train_acc: 0.718750\n",
      "EPOCH 63 STEP 6700 : train_loss: 1.229310 train_acc: 0.675781\n",
      "EPOCH 64 STEP 6800 : train_loss: 1.226048 train_acc: 0.675781\n",
      "EPOCH 65 STEP 6900 : train_loss: 1.233739 train_acc: 0.664062\n",
      "EPOCH 66 STEP 7000 : train_loss: 1.176451 train_acc: 0.734375\n",
      "EPOCH 67 STEP 7100 : train_loss: 1.206362 train_acc: 0.699219\n",
      "EPOCH 68 STEP 7200 : train_loss: 1.184076 train_acc: 0.718750\n",
      "EPOCH 69 STEP 7300 : train_loss: 1.204144 train_acc: 0.699219\n",
      "EPOCH 70 STEP 7400 : train_loss: 1.208571 train_acc: 0.683594\n",
      "EPOCH 71 STEP 7500 : train_loss: 1.219164 train_acc: 0.691406\n",
      "EPOCH 72 STEP 7600 : train_loss: 1.246592 train_acc: 0.656250\n",
      "EPOCH 73 STEP 7700 : train_loss: 1.209318 train_acc: 0.695312\n",
      "EPOCH 74 STEP 7800 : train_loss: 1.204563 train_acc: 0.710938\n",
      "EPOCH 75 STEP 7900 : train_loss: 1.203919 train_acc: 0.699219\n",
      "EPOCH 76 STEP 8000 : train_loss: 1.229270 train_acc: 0.679688\n",
      "EPOCH 77 STEP 8100 : train_loss: 1.181240 train_acc: 0.714844\n",
      "EPOCH 78 STEP 8200 : train_loss: 1.184686 train_acc: 0.718750\n",
      "EPOCH 79 STEP 8300 : train_loss: 1.193677 train_acc: 0.714844\n",
      "EPOCH 79 STEP 8400 : train_loss: 1.204757 train_acc: 0.695238\n",
      "EPOCH 80 STEP 8500 : train_loss: 1.187185 train_acc: 0.710938\n",
      "EPOCH 81 STEP 8600 : train_loss: 1.202627 train_acc: 0.699219\n",
      "EPOCH 82 STEP 8700 : train_loss: 1.233944 train_acc: 0.675781\n",
      "EPOCH 83 STEP 8800 : train_loss: 1.165768 train_acc: 0.738281\n",
      "EPOCH 84 STEP 8900 : train_loss: 1.202191 train_acc: 0.703125\n",
      "EPOCH 85 STEP 9000 : train_loss: 1.226918 train_acc: 0.667969\n",
      "EPOCH 86 STEP 9100 : train_loss: 1.174627 train_acc: 0.734375\n",
      "EPOCH 87 STEP 9200 : train_loss: 1.227447 train_acc: 0.679688\n",
      "EPOCH 88 STEP 9300 : train_loss: 1.240211 train_acc: 0.664062\n",
      "EPOCH 89 STEP 9400 : train_loss: 1.184564 train_acc: 0.710938\n",
      "EPOCH 90 STEP 9500 : train_loss: 1.174893 train_acc: 0.734375\n",
      "EPOCH 91 STEP 9600 : train_loss: 1.235894 train_acc: 0.664062\n",
      "EPOCH 92 STEP 9700 : train_loss: 1.232410 train_acc: 0.671875\n",
      "EPOCH 93 STEP 9800 : train_loss: 1.199721 train_acc: 0.699219\n",
      "EPOCH 94 STEP 9900 : train_loss: 1.206916 train_acc: 0.691406\n",
      "EPOCH 95 STEP 10000 : train_loss: 1.205624 train_acc: 0.699219\n",
      "EPOCH 96 STEP 10100 : train_loss: 1.192566 train_acc: 0.714844\n",
      "EPOCH 97 STEP 10200 : train_loss: 1.139597 train_acc: 0.765625\n",
      "EPOCH 98 STEP 10300 : train_loss: 1.185265 train_acc: 0.718750\n",
      "EPOCH 99 STEP 10400 : train_loss: 1.205201 train_acc: 0.695312\n",
      "EPOCH 99 STEP 10500 : train_loss: 1.172916 train_acc: 0.733333\n",
      "EPOCH 100 STEP 10600 : train_loss: 1.176467 train_acc: 0.726562\n",
      "EPOCH 101 STEP 10700 : train_loss: 1.213982 train_acc: 0.679688\n",
      "EPOCH 102 STEP 10800 : train_loss: 1.229334 train_acc: 0.683594\n",
      "EPOCH 103 STEP 10900 : train_loss: 1.149388 train_acc: 0.757812\n",
      "EPOCH 104 STEP 11000 : train_loss: 1.227810 train_acc: 0.671875\n",
      "EPOCH 105 STEP 11100 : train_loss: 1.200565 train_acc: 0.695312\n",
      "EPOCH 106 STEP 11200 : train_loss: 1.247931 train_acc: 0.644531\n",
      "EPOCH 107 STEP 11300 : train_loss: 1.248368 train_acc: 0.656250\n",
      "EPOCH 108 STEP 11400 : train_loss: 1.226625 train_acc: 0.679688\n",
      "EPOCH 109 STEP 11500 : train_loss: 1.222571 train_acc: 0.679688\n",
      "EPOCH 110 STEP 11600 : train_loss: 1.189497 train_acc: 0.710938\n",
      "EPOCH 111 STEP 11700 : train_loss: 1.185748 train_acc: 0.714844\n",
      "EPOCH 112 STEP 11800 : train_loss: 1.148074 train_acc: 0.750000\n",
      "EPOCH 113 STEP 11900 : train_loss: 1.210220 train_acc: 0.687500\n",
      "EPOCH 114 STEP 12000 : train_loss: 1.168752 train_acc: 0.742188\n",
      "EPOCH 115 STEP 12100 : train_loss: 1.245531 train_acc: 0.652344\n",
      "EPOCH 116 STEP 12200 : train_loss: 1.174760 train_acc: 0.730469\n",
      "EPOCH 117 STEP 12300 : train_loss: 1.189245 train_acc: 0.714844\n",
      "EPOCH 118 STEP 12400 : train_loss: 1.191136 train_acc: 0.714844\n",
      "EPOCH 119 STEP 12500 : train_loss: 1.205942 train_acc: 0.695312\n",
      "EPOCH 119 STEP 12600 : train_loss: 1.204582 train_acc: 0.704762\n",
      "EPOCH 120 STEP 12700 : train_loss: 1.207324 train_acc: 0.687500\n",
      "EPOCH 121 STEP 12800 : train_loss: 1.176398 train_acc: 0.726562\n",
      "EPOCH 122 STEP 12900 : train_loss: 1.198598 train_acc: 0.707031\n",
      "EPOCH 123 STEP 13000 : train_loss: 1.213175 train_acc: 0.683594\n",
      "EPOCH 124 STEP 13100 : train_loss: 1.190871 train_acc: 0.707031\n",
      "EPOCH 125 STEP 13200 : train_loss: 1.207562 train_acc: 0.699219\n",
      "EPOCH 126 STEP 13300 : train_loss: 1.226212 train_acc: 0.683594\n",
      "EPOCH 127 STEP 13400 : train_loss: 1.164749 train_acc: 0.738281\n",
      "EPOCH 128 STEP 13500 : train_loss: 1.211169 train_acc: 0.691406\n",
      "EPOCH 129 STEP 13600 : train_loss: 1.201921 train_acc: 0.703125\n",
      "EPOCH 130 STEP 13700 : train_loss: 1.161909 train_acc: 0.742188\n",
      "EPOCH 131 STEP 13800 : train_loss: 1.170529 train_acc: 0.742188\n",
      "EPOCH 132 STEP 13900 : train_loss: 1.240878 train_acc: 0.656250\n",
      "EPOCH 133 STEP 14000 : train_loss: 1.219264 train_acc: 0.675781\n",
      "EPOCH 134 STEP 14100 : train_loss: 1.165205 train_acc: 0.734375\n",
      "EPOCH 135 STEP 14200 : train_loss: 1.164554 train_acc: 0.738281\n",
      "EPOCH 136 STEP 14300 : train_loss: 1.197916 train_acc: 0.695312\n",
      "EPOCH 137 STEP 14400 : train_loss: 1.243388 train_acc: 0.664062\n",
      "EPOCH 138 STEP 14500 : train_loss: 1.197565 train_acc: 0.710938\n",
      "EPOCH 139 STEP 14600 : train_loss: 1.150266 train_acc: 0.750000\n",
      "EPOCH 139 STEP 14700 : train_loss: 1.188596 train_acc: 0.714286\n",
      "EPOCH 140 STEP 14800 : train_loss: 1.220211 train_acc: 0.679688\n",
      "EPOCH 141 STEP 14900 : train_loss: 1.258508 train_acc: 0.648438\n",
      "EPOCH 142 STEP 15000 : train_loss: 1.174897 train_acc: 0.726562\n",
      "EPOCH 143 STEP 15100 : train_loss: 1.200793 train_acc: 0.699219\n",
      "EPOCH 144 STEP 15200 : train_loss: 1.181065 train_acc: 0.726562\n",
      "EPOCH 145 STEP 15300 : train_loss: 1.191352 train_acc: 0.710938\n",
      "EPOCH 146 STEP 15400 : train_loss: 1.215859 train_acc: 0.687500\n",
      "EPOCH 147 STEP 15500 : train_loss: 1.277411 train_acc: 0.628906\n",
      "EPOCH 148 STEP 15600 : train_loss: 1.215578 train_acc: 0.687500\n",
      "EPOCH 149 STEP 15700 : train_loss: 1.234244 train_acc: 0.675781\n",
      "EPOCH 150 STEP 15800 : train_loss: 1.204026 train_acc: 0.699219\n",
      "EPOCH 151 STEP 15900 : train_loss: 1.183809 train_acc: 0.718750\n",
      "EPOCH 152 STEP 16000 : train_loss: 1.217402 train_acc: 0.679688\n",
      "EPOCH 153 STEP 16100 : train_loss: 1.189089 train_acc: 0.714844\n",
      "EPOCH 154 STEP 16200 : train_loss: 1.259963 train_acc: 0.640625\n",
      "EPOCH 155 STEP 16300 : train_loss: 1.242778 train_acc: 0.664062\n",
      "EPOCH 156 STEP 16400 : train_loss: 1.205016 train_acc: 0.699219\n",
      "EPOCH 157 STEP 16500 : train_loss: 1.211620 train_acc: 0.687500\n",
      "EPOCH 158 STEP 16600 : train_loss: 1.214316 train_acc: 0.695312\n",
      "EPOCH 159 STEP 16700 : train_loss: 1.203743 train_acc: 0.695312\n",
      "EPOCH 159 STEP 16800 : train_loss: 1.257844 train_acc: 0.638095\n",
      "EPOCH 160 STEP 16900 : train_loss: 1.173344 train_acc: 0.734375\n",
      "EPOCH 161 STEP 17000 : train_loss: 1.207674 train_acc: 0.695312\n",
      "EPOCH 162 STEP 17100 : train_loss: 1.174641 train_acc: 0.726562\n",
      "EPOCH 163 STEP 17200 : train_loss: 1.194229 train_acc: 0.710938\n",
      "EPOCH 164 STEP 17300 : train_loss: 1.153261 train_acc: 0.757812\n",
      "EPOCH 165 STEP 17400 : train_loss: 1.202962 train_acc: 0.703125\n",
      "EPOCH 166 STEP 17500 : train_loss: 1.209616 train_acc: 0.691406\n",
      "EPOCH 167 STEP 17600 : train_loss: 1.149838 train_acc: 0.761719\n",
      "EPOCH 168 STEP 17700 : train_loss: 1.235817 train_acc: 0.667969\n",
      "EPOCH 169 STEP 17800 : train_loss: 1.218576 train_acc: 0.683594\n",
      "EPOCH 170 STEP 17900 : train_loss: 1.191369 train_acc: 0.718750\n",
      "EPOCH 171 STEP 18000 : train_loss: 1.168228 train_acc: 0.734375\n",
      "EPOCH 172 STEP 18100 : train_loss: 1.180636 train_acc: 0.722656\n",
      "EPOCH 173 STEP 18200 : train_loss: 1.185169 train_acc: 0.718750\n",
      "EPOCH 174 STEP 18300 : train_loss: 1.186330 train_acc: 0.714844\n",
      "EPOCH 175 STEP 18400 : train_loss: 1.185915 train_acc: 0.710938\n",
      "EPOCH 176 STEP 18500 : train_loss: 1.235306 train_acc: 0.667969\n",
      "EPOCH 177 STEP 18600 : train_loss: 1.157032 train_acc: 0.742188\n",
      "EPOCH 178 STEP 18700 : train_loss: 1.162007 train_acc: 0.746094\n",
      "EPOCH 179 STEP 18800 : train_loss: 1.171321 train_acc: 0.730469\n",
      "EPOCH 179 STEP 18900 : train_loss: 1.179862 train_acc: 0.723810\n",
      "EPOCH 180 STEP 19000 : train_loss: 1.215371 train_acc: 0.691406\n",
      "EPOCH 181 STEP 19100 : train_loss: 1.198807 train_acc: 0.703125\n",
      "EPOCH 182 STEP 19200 : train_loss: 1.185553 train_acc: 0.714844\n",
      "EPOCH 183 STEP 19300 : train_loss: 1.237609 train_acc: 0.664062\n",
      "EPOCH 184 STEP 19400 : train_loss: 1.222699 train_acc: 0.675781\n",
      "EPOCH 185 STEP 19500 : train_loss: 1.181418 train_acc: 0.718750\n",
      "EPOCH 186 STEP 19600 : train_loss: 1.231400 train_acc: 0.675781\n",
      "EPOCH 187 STEP 19700 : train_loss: 1.219025 train_acc: 0.687500\n",
      "EPOCH 188 STEP 19800 : train_loss: 1.229737 train_acc: 0.671875\n",
      "EPOCH 189 STEP 19900 : train_loss: 1.206378 train_acc: 0.699219\n",
      "EPOCH 190 STEP 20000 : train_loss: 1.204727 train_acc: 0.695312\n",
      "EPOCH 191 STEP 20100 : train_loss: 1.232011 train_acc: 0.671875\n",
      "EPOCH 192 STEP 20200 : train_loss: 1.171926 train_acc: 0.734375\n",
      "EPOCH 193 STEP 20300 : train_loss: 1.178335 train_acc: 0.726562\n",
      "EPOCH 194 STEP 20400 : train_loss: 1.210827 train_acc: 0.687500\n",
      "EPOCH 195 STEP 20500 : train_loss: 1.154217 train_acc: 0.746094\n",
      "EPOCH 196 STEP 20600 : train_loss: 1.216236 train_acc: 0.687500\n",
      "EPOCH 197 STEP 20700 : train_loss: 1.196025 train_acc: 0.703125\n",
      "EPOCH 198 STEP 20800 : train_loss: 1.167012 train_acc: 0.738281\n",
      "EPOCH 199 STEP 20900 : train_loss: 1.194623 train_acc: 0.710938\n",
      "EPOCH 199 STEP 21000 : train_loss: 1.259055 train_acc: 0.647619\n",
      "EPOCH 200 STEP 21100 : train_loss: 1.215721 train_acc: 0.687500\n",
      "EPOCH 201 STEP 21200 : train_loss: 1.217775 train_acc: 0.683594\n",
      "EPOCH 202 STEP 21300 : train_loss: 1.244259 train_acc: 0.656250\n",
      "EPOCH 203 STEP 21400 : train_loss: 1.195815 train_acc: 0.707031\n",
      "EPOCH 204 STEP 21500 : train_loss: 1.191587 train_acc: 0.710938\n",
      "EPOCH 205 STEP 21600 : train_loss: 1.231176 train_acc: 0.671875\n",
      "EPOCH 206 STEP 21700 : train_loss: 1.224798 train_acc: 0.675781\n",
      "EPOCH 207 STEP 21800 : train_loss: 1.163640 train_acc: 0.738281\n",
      "EPOCH 208 STEP 21900 : train_loss: 1.211385 train_acc: 0.699219\n",
      "EPOCH 209 STEP 22000 : train_loss: 1.200081 train_acc: 0.703125\n",
      "EPOCH 210 STEP 22100 : train_loss: 1.190910 train_acc: 0.714844\n",
      "EPOCH 211 STEP 22200 : train_loss: 1.199441 train_acc: 0.703125\n",
      "EPOCH 212 STEP 22300 : train_loss: 1.177433 train_acc: 0.722656\n",
      "EPOCH 213 STEP 22400 : train_loss: 1.193680 train_acc: 0.710938\n",
      "EPOCH 214 STEP 22500 : train_loss: 1.228790 train_acc: 0.679688\n",
      "EPOCH 215 STEP 22600 : train_loss: 1.231887 train_acc: 0.667969\n",
      "EPOCH 216 STEP 22700 : train_loss: 1.194369 train_acc: 0.710938\n",
      "EPOCH 217 STEP 22800 : train_loss: 1.205253 train_acc: 0.699219\n",
      "EPOCH 218 STEP 22900 : train_loss: 1.150211 train_acc: 0.750000\n",
      "EPOCH 219 STEP 23000 : train_loss: 1.181808 train_acc: 0.707031\n",
      "EPOCH 219 STEP 23100 : train_loss: 1.216250 train_acc: 0.695238\n",
      "EPOCH 220 STEP 23200 : train_loss: 1.217463 train_acc: 0.687500\n",
      "EPOCH 221 STEP 23300 : train_loss: 1.187289 train_acc: 0.718750\n",
      "EPOCH 222 STEP 23400 : train_loss: 1.161495 train_acc: 0.738281\n",
      "EPOCH 223 STEP 23500 : train_loss: 1.177920 train_acc: 0.726562\n",
      "EPOCH 224 STEP 23600 : train_loss: 1.201399 train_acc: 0.691406\n",
      "EPOCH 225 STEP 23700 : train_loss: 1.227772 train_acc: 0.675781\n",
      "EPOCH 226 STEP 23800 : train_loss: 1.183242 train_acc: 0.718750\n",
      "EPOCH 227 STEP 23900 : train_loss: 1.229324 train_acc: 0.667969\n",
      "EPOCH 228 STEP 24000 : train_loss: 1.115492 train_acc: 0.789062\n",
      "EPOCH 229 STEP 24100 : train_loss: 1.208640 train_acc: 0.691406\n",
      "EPOCH 230 STEP 24200 : train_loss: 1.188617 train_acc: 0.714844\n",
      "EPOCH 231 STEP 24300 : train_loss: 1.181904 train_acc: 0.730469\n",
      "EPOCH 232 STEP 24400 : train_loss: 1.192961 train_acc: 0.703125\n",
      "EPOCH 233 STEP 24500 : train_loss: 1.176714 train_acc: 0.722656\n",
      "EPOCH 234 STEP 24600 : train_loss: 1.214257 train_acc: 0.687500\n",
      "EPOCH 235 STEP 24700 : train_loss: 1.195991 train_acc: 0.707031\n",
      "EPOCH 236 STEP 24800 : train_loss: 1.178066 train_acc: 0.722656\n",
      "EPOCH 237 STEP 24900 : train_loss: 1.210421 train_acc: 0.691406\n",
      "EPOCH 238 STEP 25000 : train_loss: 1.225702 train_acc: 0.679688\n",
      "EPOCH 239 STEP 25100 : train_loss: 1.192837 train_acc: 0.714844\n",
      "EPOCH 239 STEP 25200 : train_loss: 1.243599 train_acc: 0.657143\n",
      "EPOCH 240 STEP 25300 : train_loss: 1.199731 train_acc: 0.695312\n",
      "EPOCH 241 STEP 25400 : train_loss: 1.172589 train_acc: 0.738281\n",
      "EPOCH 242 STEP 25500 : train_loss: 1.214440 train_acc: 0.687500\n",
      "EPOCH 243 STEP 25600 : train_loss: 1.226173 train_acc: 0.683594\n",
      "EPOCH 244 STEP 25700 : train_loss: 1.189521 train_acc: 0.718750\n",
      "EPOCH 245 STEP 25800 : train_loss: 1.212373 train_acc: 0.691406\n",
      "EPOCH 246 STEP 25900 : train_loss: 1.193677 train_acc: 0.707031\n",
      "EPOCH 247 STEP 26000 : train_loss: 1.213043 train_acc: 0.691406\n",
      "EPOCH 248 STEP 26100 : train_loss: 1.155179 train_acc: 0.742188\n",
      "EPOCH 249 STEP 26200 : train_loss: 1.168687 train_acc: 0.730469\n",
      "EPOCH 250 STEP 26300 : train_loss: 1.184112 train_acc: 0.722656\n",
      "EPOCH 251 STEP 26400 : train_loss: 1.215721 train_acc: 0.691406\n",
      "EPOCH 252 STEP 26500 : train_loss: 1.183627 train_acc: 0.722656\n",
      "EPOCH 253 STEP 26600 : train_loss: 1.261539 train_acc: 0.644531\n",
      "EPOCH 254 STEP 26700 : train_loss: 1.200197 train_acc: 0.707031\n",
      "EPOCH 255 STEP 26800 : train_loss: 1.210891 train_acc: 0.695312\n",
      "EPOCH 256 STEP 26900 : train_loss: 1.199938 train_acc: 0.710938\n",
      "EPOCH 257 STEP 27000 : train_loss: 1.218732 train_acc: 0.687500\n",
      "EPOCH 258 STEP 27100 : train_loss: 1.191270 train_acc: 0.710938\n",
      "EPOCH 259 STEP 27200 : train_loss: 1.174464 train_acc: 0.738281\n",
      "EPOCH 259 STEP 27300 : train_loss: 1.184856 train_acc: 0.704762\n",
      "EPOCH 260 STEP 27400 : train_loss: 1.255516 train_acc: 0.648438\n",
      "EPOCH 261 STEP 27500 : train_loss: 1.191311 train_acc: 0.707031\n",
      "EPOCH 262 STEP 27600 : train_loss: 1.197595 train_acc: 0.710938\n",
      "EPOCH 263 STEP 27700 : train_loss: 1.205627 train_acc: 0.695312\n",
      "EPOCH 264 STEP 27800 : train_loss: 1.198419 train_acc: 0.699219\n",
      "EPOCH 265 STEP 27900 : train_loss: 1.183598 train_acc: 0.718750\n",
      "EPOCH 266 STEP 28000 : train_loss: 1.195464 train_acc: 0.707031\n",
      "EPOCH 267 STEP 28100 : train_loss: 1.253088 train_acc: 0.652344\n",
      "EPOCH 268 STEP 28200 : train_loss: 1.196144 train_acc: 0.710938\n",
      "EPOCH 269 STEP 28300 : train_loss: 1.212339 train_acc: 0.691406\n",
      "EPOCH 270 STEP 28400 : train_loss: 1.189073 train_acc: 0.710938\n",
      "EPOCH 271 STEP 28500 : train_loss: 1.264692 train_acc: 0.632812\n",
      "EPOCH 272 STEP 28600 : train_loss: 1.222258 train_acc: 0.679688\n",
      "EPOCH 273 STEP 28700 : train_loss: 1.232834 train_acc: 0.679688\n",
      "EPOCH 274 STEP 28800 : train_loss: 1.207715 train_acc: 0.695312\n",
      "EPOCH 275 STEP 28900 : train_loss: 1.178584 train_acc: 0.722656\n",
      "EPOCH 276 STEP 29000 : train_loss: 1.203867 train_acc: 0.699219\n",
      "EPOCH 277 STEP 29100 : train_loss: 1.176340 train_acc: 0.730469\n",
      "EPOCH 278 STEP 29200 : train_loss: 1.198957 train_acc: 0.703125\n",
      "EPOCH 279 STEP 29300 : train_loss: 1.163060 train_acc: 0.742188\n",
      "EPOCH 279 STEP 29400 : train_loss: 1.218889 train_acc: 0.685714\n",
      "EPOCH 280 STEP 29500 : train_loss: 1.171297 train_acc: 0.730469\n",
      "EPOCH 281 STEP 29600 : train_loss: 1.215659 train_acc: 0.679688\n",
      "EPOCH 282 STEP 29700 : train_loss: 1.222286 train_acc: 0.679688\n",
      "EPOCH 283 STEP 29800 : train_loss: 1.184242 train_acc: 0.726562\n",
      "EPOCH 284 STEP 29900 : train_loss: 1.176472 train_acc: 0.726562\n",
      "EPOCH 285 STEP 30000 : train_loss: 1.212350 train_acc: 0.695312\n",
      "EPOCH 286 STEP 30100 : train_loss: 1.175237 train_acc: 0.730469\n",
      "EPOCH 287 STEP 30200 : train_loss: 1.163505 train_acc: 0.742188\n",
      "EPOCH 288 STEP 30300 : train_loss: 1.195580 train_acc: 0.710938\n",
      "EPOCH 289 STEP 30400 : train_loss: 1.155405 train_acc: 0.750000\n",
      "EPOCH 290 STEP 30500 : train_loss: 1.154333 train_acc: 0.753906\n",
      "EPOCH 291 STEP 30600 : train_loss: 1.177673 train_acc: 0.726562\n",
      "EPOCH 292 STEP 30700 : train_loss: 1.212975 train_acc: 0.691406\n",
      "EPOCH 293 STEP 30800 : train_loss: 1.200705 train_acc: 0.703125\n",
      "EPOCH 294 STEP 30900 : train_loss: 1.200420 train_acc: 0.703125\n",
      "EPOCH 295 STEP 31000 : train_loss: 1.191435 train_acc: 0.718750\n",
      "EPOCH 296 STEP 31100 : train_loss: 1.165257 train_acc: 0.746094\n",
      "EPOCH 297 STEP 31200 : train_loss: 1.172333 train_acc: 0.734375\n",
      "EPOCH 298 STEP 31300 : train_loss: 1.149294 train_acc: 0.753906\n",
      "EPOCH 299 STEP 31400 : train_loss: 1.211898 train_acc: 0.691406\n",
      "EPOCH 299 STEP 31500 : train_loss: 1.136145 train_acc: 0.761905\n",
      "EPOCH 300 STEP 31600 : train_loss: 1.166681 train_acc: 0.738281\n",
      "EPOCH 301 STEP 31700 : train_loss: 1.191208 train_acc: 0.714844\n",
      "EPOCH 302 STEP 31800 : train_loss: 1.193036 train_acc: 0.710938\n",
      "EPOCH 303 STEP 31900 : train_loss: 1.205242 train_acc: 0.695312\n",
      "EPOCH 304 STEP 32000 : train_loss: 1.229228 train_acc: 0.675781\n",
      "EPOCH 305 STEP 32100 : train_loss: 1.143866 train_acc: 0.765625\n",
      "EPOCH 306 STEP 32200 : train_loss: 1.233903 train_acc: 0.671875\n",
      "EPOCH 307 STEP 32300 : train_loss: 1.202321 train_acc: 0.703125\n",
      "EPOCH 308 STEP 32400 : train_loss: 1.211110 train_acc: 0.691406\n",
      "EPOCH 309 STEP 32500 : train_loss: 1.217196 train_acc: 0.687500\n",
      "EPOCH 310 STEP 32600 : train_loss: 1.206737 train_acc: 0.695312\n",
      "EPOCH 311 STEP 32700 : train_loss: 1.167932 train_acc: 0.734375\n",
      "EPOCH 312 STEP 32800 : train_loss: 1.229320 train_acc: 0.671875\n",
      "EPOCH 313 STEP 32900 : train_loss: 1.180750 train_acc: 0.726562\n",
      "EPOCH 314 STEP 33000 : train_loss: 1.150461 train_acc: 0.750000\n",
      "EPOCH 315 STEP 33100 : train_loss: 1.192092 train_acc: 0.714844\n",
      "EPOCH 316 STEP 33200 : train_loss: 1.192040 train_acc: 0.703125\n",
      "EPOCH 317 STEP 33300 : train_loss: 1.166985 train_acc: 0.742188\n",
      "EPOCH 318 STEP 33400 : train_loss: 1.286128 train_acc: 0.613281\n",
      "EPOCH 319 STEP 33500 : train_loss: 1.178106 train_acc: 0.726562\n",
      "EPOCH 319 STEP 33600 : train_loss: 1.268220 train_acc: 0.638095\n",
      "EPOCH 320 STEP 33700 : train_loss: 1.189200 train_acc: 0.718750\n",
      "EPOCH 321 STEP 33800 : train_loss: 1.176801 train_acc: 0.722656\n",
      "EPOCH 322 STEP 33900 : train_loss: 1.197726 train_acc: 0.710938\n",
      "EPOCH 323 STEP 34000 : train_loss: 1.214048 train_acc: 0.687500\n",
      "EPOCH 324 STEP 34100 : train_loss: 1.200554 train_acc: 0.703125\n",
      "EPOCH 325 STEP 34200 : train_loss: 1.162890 train_acc: 0.734375\n",
      "EPOCH 326 STEP 34300 : train_loss: 1.191230 train_acc: 0.714844\n",
      "EPOCH 327 STEP 34400 : train_loss: 1.198912 train_acc: 0.703125\n",
      "EPOCH 328 STEP 34500 : train_loss: 1.174420 train_acc: 0.726562\n",
      "EPOCH 329 STEP 34600 : train_loss: 1.204129 train_acc: 0.695312\n",
      "EPOCH 330 STEP 34700 : train_loss: 1.189754 train_acc: 0.718750\n",
      "EPOCH 331 STEP 34800 : train_loss: 1.169844 train_acc: 0.738281\n",
      "EPOCH 332 STEP 34900 : train_loss: 1.195798 train_acc: 0.699219\n",
      "EPOCH 333 STEP 35000 : train_loss: 1.173475 train_acc: 0.730469\n",
      "EPOCH 334 STEP 35100 : train_loss: 1.180262 train_acc: 0.722656\n",
      "EPOCH 335 STEP 35200 : train_loss: 1.169421 train_acc: 0.730469\n",
      "EPOCH 336 STEP 35300 : train_loss: 1.182061 train_acc: 0.722656\n",
      "EPOCH 337 STEP 35400 : train_loss: 1.171045 train_acc: 0.738281\n",
      "EPOCH 338 STEP 35500 : train_loss: 1.173149 train_acc: 0.730469\n",
      "EPOCH 339 STEP 35600 : train_loss: 1.245516 train_acc: 0.656250\n",
      "EPOCH 339 STEP 35700 : train_loss: 1.189951 train_acc: 0.714286\n",
      "EPOCH 340 STEP 35800 : train_loss: 1.198843 train_acc: 0.707031\n",
      "EPOCH 341 STEP 35900 : train_loss: 1.185692 train_acc: 0.710938\n",
      "EPOCH 342 STEP 36000 : train_loss: 1.207582 train_acc: 0.699219\n",
      "EPOCH 343 STEP 36100 : train_loss: 1.207249 train_acc: 0.699219\n",
      "EPOCH 344 STEP 36200 : train_loss: 1.243769 train_acc: 0.660156\n",
      "EPOCH 345 STEP 36300 : train_loss: 1.209327 train_acc: 0.695312\n",
      "EPOCH 346 STEP 36400 : train_loss: 1.137498 train_acc: 0.769531\n",
      "EPOCH 347 STEP 36500 : train_loss: 1.182079 train_acc: 0.718750\n",
      "EPOCH 348 STEP 36600 : train_loss: 1.238914 train_acc: 0.656250\n",
      "EPOCH 349 STEP 36700 : train_loss: 1.239885 train_acc: 0.660156\n",
      "EPOCH 350 STEP 36800 : train_loss: 1.218260 train_acc: 0.679688\n",
      "EPOCH 351 STEP 36900 : train_loss: 1.199158 train_acc: 0.707031\n",
      "EPOCH 352 STEP 37000 : train_loss: 1.222429 train_acc: 0.683594\n",
      "EPOCH 353 STEP 37100 : train_loss: 1.171036 train_acc: 0.734375\n",
      "EPOCH 354 STEP 37200 : train_loss: 1.179870 train_acc: 0.722656\n",
      "EPOCH 355 STEP 37300 : train_loss: 1.211961 train_acc: 0.687500\n",
      "EPOCH 356 STEP 37400 : train_loss: 1.166185 train_acc: 0.742188\n",
      "EPOCH 357 STEP 37500 : train_loss: 1.171979 train_acc: 0.734375\n",
      "EPOCH 358 STEP 37600 : train_loss: 1.166112 train_acc: 0.738281\n",
      "EPOCH 359 STEP 37700 : train_loss: 1.131137 train_acc: 0.769531\n",
      "EPOCH 359 STEP 37800 : train_loss: 1.203439 train_acc: 0.704762\n",
      "EPOCH 360 STEP 37900 : train_loss: 1.214696 train_acc: 0.691406\n",
      "EPOCH 361 STEP 38000 : train_loss: 1.190111 train_acc: 0.718750\n",
      "EPOCH 362 STEP 38100 : train_loss: 1.206989 train_acc: 0.695312\n",
      "EPOCH 363 STEP 38200 : train_loss: 1.167784 train_acc: 0.734375\n",
      "EPOCH 364 STEP 38300 : train_loss: 1.179598 train_acc: 0.722656\n",
      "EPOCH 365 STEP 38400 : train_loss: 1.186775 train_acc: 0.714844\n",
      "EPOCH 366 STEP 38500 : train_loss: 1.123725 train_acc: 0.773438\n",
      "EPOCH 367 STEP 38600 : train_loss: 1.209807 train_acc: 0.695312\n",
      "EPOCH 368 STEP 38700 : train_loss: 1.210675 train_acc: 0.691406\n",
      "EPOCH 369 STEP 38800 : train_loss: 1.146891 train_acc: 0.750000\n",
      "EPOCH 370 STEP 38900 : train_loss: 1.192280 train_acc: 0.707031\n",
      "EPOCH 371 STEP 39000 : train_loss: 1.193432 train_acc: 0.714844\n",
      "EPOCH 372 STEP 39100 : train_loss: 1.185551 train_acc: 0.722656\n",
      "EPOCH 373 STEP 39200 : train_loss: 1.192584 train_acc: 0.710938\n",
      "EPOCH 374 STEP 39300 : train_loss: 1.221926 train_acc: 0.675781\n",
      "EPOCH 375 STEP 39400 : train_loss: 1.161410 train_acc: 0.742188\n",
      "EPOCH 376 STEP 39500 : train_loss: 1.184363 train_acc: 0.714844\n",
      "EPOCH 377 STEP 39600 : train_loss: 1.167389 train_acc: 0.738281\n",
      "EPOCH 378 STEP 39700 : train_loss: 1.174054 train_acc: 0.726562\n",
      "EPOCH 379 STEP 39800 : train_loss: 1.172657 train_acc: 0.730469\n",
      "EPOCH 379 STEP 39900 : train_loss: 1.184911 train_acc: 0.714286\n",
      "EPOCH 380 STEP 40000 : train_loss: 1.195668 train_acc: 0.707031\n",
      "EPOCH 381 STEP 40100 : train_loss: 1.223051 train_acc: 0.679688\n",
      "EPOCH 382 STEP 40200 : train_loss: 1.193009 train_acc: 0.714844\n",
      "EPOCH 383 STEP 40300 : train_loss: 1.209025 train_acc: 0.695312\n",
      "EPOCH 384 STEP 40400 : train_loss: 1.163536 train_acc: 0.738281\n",
      "EPOCH 385 STEP 40500 : train_loss: 1.236431 train_acc: 0.664062\n",
      "EPOCH 386 STEP 40600 : train_loss: 1.173385 train_acc: 0.730469\n",
      "EPOCH 387 STEP 40700 : train_loss: 1.232348 train_acc: 0.667969\n",
      "EPOCH 388 STEP 40800 : train_loss: 1.222308 train_acc: 0.679688\n",
      "EPOCH 389 STEP 40900 : train_loss: 1.174639 train_acc: 0.730469\n",
      "EPOCH 390 STEP 41000 : train_loss: 1.183333 train_acc: 0.718750\n",
      "EPOCH 391 STEP 41100 : train_loss: 1.219298 train_acc: 0.683594\n",
      "EPOCH 392 STEP 41200 : train_loss: 1.219146 train_acc: 0.679688\n",
      "EPOCH 393 STEP 41300 : train_loss: 1.209813 train_acc: 0.699219\n",
      "EPOCH 394 STEP 41400 : train_loss: 1.170287 train_acc: 0.734375\n",
      "EPOCH 395 STEP 41500 : train_loss: 1.193675 train_acc: 0.710938\n",
      "EPOCH 396 STEP 41600 : train_loss: 1.227783 train_acc: 0.667969\n",
      "EPOCH 397 STEP 41700 : train_loss: 1.261467 train_acc: 0.640625\n",
      "EPOCH 398 STEP 41800 : train_loss: 1.198262 train_acc: 0.703125\n",
      "EPOCH 399 STEP 41900 : train_loss: 1.217278 train_acc: 0.679688\n",
      "EPOCH 399 STEP 42000 : train_loss: 1.149488 train_acc: 0.761905\n"
     ]
    }
   ],
   "source": [
    "step = run_train(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
